# webscrap
# Web Scraping Loan Service Providers using BeautifulSoup

This project is a web scraping script that extracts details of loan service providers from the Indian Yellow Pages website using Python, the requests library, and BeautifulSoup for parsing HTML. The extracted data is then stored in an Excel file using the pandas library.

*Input :-*

The script prompts the user to enter the name of a city.
It dynamically constructs the URL to scrape loan service provider data from the Indian Yellow Pages website for the specified city.
*Output :-*

If the request is successful(HTTP Status Code= 200), the script extracts the following details for each listed loan service provider:
Title (Company Name), Link (Company Profile URL), Address, Rating.

The extracted data is saved in an Excel file named loan_service_providers_.xlsx

If the request fails, an error message with the HTTP status code is displayed.(e.g. 403)

*Target Website :-* https://www.indianyellowpages.com/

How to Run the Script

Install the required Python libraries:

pip install requests beautifulsoup4 pandas

Run the script:

python script.py

Enter the desired city name when prompted.

The extracted data will be saved in an Excel file in the same directory as the script.
Enter the city name: Mumbai
Data saved to loan_service_providers_Mumbai.xlsx



bash 
```
/multi_agent_system
|
â”œâ”€â”€ ðŸ“‚ agents/                # Directory for all specialized agents
â”‚   â”œâ”€â”€ __init__.py         # Makes the folder a Python package
â”‚   â”œâ”€â”€ arxiv_agent.py      # Logic for the ArXiv search agent
â”‚   â”œâ”€â”€ pdf_rag_agent.py    # Logic for the PDF RAG agent
â”‚   â””â”€â”€ web_search_agent.py   # Logic for the web search agent
|
â”œâ”€â”€ ðŸ“‚ domain_pdfs/           # Contains sample PDFs for testing
â”‚   â””â”€â”€ ...
|
â”œâ”€â”€ ðŸ“‚ logs/                  # Stores logs from the system
â”‚   â””â”€â”€ controller_log.jsonl # Logs every decision made by the Controller
|
â”œâ”€â”€ ðŸ“‚ uploads/               # Temporary storage for user-uploaded PDFs
â”‚   â””â”€â”€ ...
|
â”œâ”€â”€ ðŸ“‚ vector_store/          # Caches the FAISS vector embeddings for PDFs
â”‚   â””â”€â”€ ...
|
â”œâ”€â”€ ðŸ“œ .env                   # Stores secret environment variables (API keys)
â”œâ”€â”€ ðŸ“œ .gitignore             # Specifies files for Git to ignore (e.g., myenv/, __pycache__/)
â”œâ”€â”€ ðŸ“œ README.md              # Project documentation
â”œâ”€â”€ ðŸ“œ app.py                 # The Streamlit frontend application
â”œâ”€â”€ ðŸ“œ controller.py           # The Controller Agent logic (the "brain")
â”œâ”€â”€ ðŸ“œ main.py                # The FastAPI backend server
â”œâ”€â”€ ðŸ“œ requirements.txt        # Lists all Python package dependencies
â””â”€â”€ ðŸ“œ utils.py               # Helper functions, like the answer synthesizer

```
